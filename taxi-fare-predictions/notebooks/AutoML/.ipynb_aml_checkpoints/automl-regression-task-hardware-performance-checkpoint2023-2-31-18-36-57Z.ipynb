{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Related links:\n",
    "- [AutoML Examples SDKv2](https://github.com/Azure/azureml-examples/tree/main/sdk/python/jobs/automl-standalone-jobs)\n",
    "- [AutomL Examples  SDKv1](https://github.com/Azure/azureml-examples/tree/main/v1/python-sdk/tutorials/automl-with-azureml)\n",
    "- [How to Configure AutoML Train v2](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# AutoML: Train \"the best\" Regression model for the Hardware dataset.\n",
    "\n",
    "**Requirements** - In order to benefit from this tutorial, you will need:\n",
    "- A basic understanding of Machine Learning\n",
    "- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F)\n",
    "- An Azure ML workspace. [Check this notebook for creating a workspace](../../../resources/workspace/workspace.ipynb) \n",
    "- A Compute Cluster. [Check this notebook to create a compute cluster](../../../resources/compute/compute.ipynb)\n",
    "- A python environment\n",
    "- Installed Azure Machine Learning Python SDK v2 - [install instructions](../../../README.md) - check the getting started section\n",
    "\n",
    "**Learning Objectives** - By the end of this tutorial, you should be able to:\n",
    "- Connect to your AML workspace from the Python SDK\n",
    "- Create an `AutoML regression Job` with the 'regression()' factory-function.\n",
    "- Train the model using AmlCompute by submitting/running the AutoML regression training job\n",
    "- Obtaing the model and score predictions with it\n",
    "\n",
    "**Motivations** - This notebook explains how to setup and run an AutoML regression job. This is one of the nine ML-tasks supported by AutoML. Other ML-tasks are 'forecasting', 'classification', 'image classification', 'image object detection', 'nlp text classification', etc.\n",
    "\n",
    "In this notebook, we go over how you can use AutoML for training a Regression model. We will use the Hardware Performance dataset to train and deploy the model to use in inference scenarios. The Regression goal is to predict the performance of certain combinations of hardware parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Connect to Azure Machine Learning Workspace\n",
    "\n",
    "The [workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section we will connect to the workspace in which the job will be run.\n",
    "\n",
    "## 1.1. Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1634852261599
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml import automl\n",
    "from azure.ai.ml import Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Configure workspace details and get a handle to the workspace\n",
    "\n",
    "To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. We will use these details in the `MLClient` from `azure.ai.ml` to get a handle to the required Azure Machine Learning workspace. We use the [default azure authentication](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) for this tutorial. Check the [configuration notebook](../../configuration.ipynb) for more details on how to configure credentials and connect to a workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1634852261884
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We could not find config.json in: . or in its parent directories. \n"
     ]
    }
   ],
   "source": [
    "credential = DefaultAzureCredential()\n",
    "ml_client = None\n",
    "try:\n",
    "    ml_client = MLClient.from_config(credential)\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    # Enter details of your AzureML workspace\n",
    "    subscription_id = \"14585b9f-5c83-4a76-8055-42149123f99f\"\n",
    "    resource_group = \"azureml_rg\"\n",
    "    workspace = \"azureml_dev_jamg\"\n",
    "    ml_client = MLClient(credential, subscription_id, resource_group, workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Azure ML Workspace information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Workspace': 'azureml_dev_jamg',\n",
       " 'Subscription ID': '14585b9f-5c83-4a76-8055-42149123f99f',\n",
       " 'Resource Group': 'azureml_rg',\n",
       " 'Location': 'eastus'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workspace = ml_client.workspaces.get(name=ml_client.workspace_name)\n",
    "\n",
    "subscription_id = ml_client.connections._subscription_id\n",
    "resource_group = workspace.resource_group\n",
    "workspace_name = ml_client.workspace_name\n",
    "\n",
    "output = {}\n",
    "output[\"Workspace\"] = workspace_name\n",
    "output[\"Subscription ID\"] = subscription_id\n",
    "output[\"Resource Group\"] = resource_group\n",
    "output[\"Location\"] = workspace.location\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MLTable with input Training Data\n",
    "\n",
    "## 2.1. Create MLTable data input\n",
    "Please make use of the MLTable files present within the data folder at the same location (in the repo) as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training MLTable defined locally, with local data to be uploaded\n",
    "my_training_data_input = Input(\n",
    "    type=AssetTypes.MLTABLE, path=\"./data/training-mltable-folder\"\n",
    ")\n",
    "\n",
    "# WITH REMOTE PATH If available already in the cloud/workspace-blob-store\n",
    "# my_training_data_input  = Input(type=AssetTypes.MLTABLE, path=\"azureml://datastores/workspaceblobstore/paths/my-regression-mltable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Compute target setup\n",
    "\n",
    "### Create or Attach existing AmlCompute\n",
    "A compute target is required to execute the Automated ML run. In this tutorial, you create AmlCompute as your training compute resource.\n",
    "\n",
    "> Note that if you have an AzureML Data Scientist role, you will not have permission to create compute resources. Talk to your workspace or IT admin to create the compute targets described in this section, if they do not already exist.\n",
    "\n",
    "#### Creation of AmlCompute takes approximately 5 minutes. \n",
    "If the AmlCompute with that name is already in your workspace this code will skip the creation process.\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target.\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "\n",
    "compute_name = \"cpu-cluster\"\n",
    "\n",
    "try:\n",
    "    _ = ml_client.compute.get(compute_name)\n",
    "    print(\"Found existing compute target.\")\n",
    "except ResourceNotFoundError:\n",
    "    print(\"Creating a new compute target...\")\n",
    "    compute_config = AmlCompute(\n",
    "        name=compute_name,\n",
    "        type=\"amlcompute\",\n",
    "        size=\"STANDARD_DS12_V2\",\n",
    "        idle_time_before_scale_down=120,\n",
    "        min_instances=0,\n",
    "        max_instances=6,\n",
    "    )\n",
    "    ml_client.begin_create_or_update(compute_config).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Configure and run the AutoML Regression training job\n",
    "In this section we will configure and run the AutoML regression job.\n",
    "\n",
    "## 4.1 Configure the job through the regression() factory function\n",
    "\n",
    "### regression() function parameters:\n",
    "\n",
    "The `regression()` factory function allows user to configure AutoML for the regression task for the most common scenarios with the following properties.\n",
    "\n",
    "- `target_column_name` - The name of the column to target for predictions. It must always be specified. This parameter is applicable to 'training_data', 'validation_data' and 'test_data'.\n",
    "- `primary_metric` - The metric that AutoML will optimize for model selection.\n",
    "- `training_data` - The data to be used for training. It should contain both training feature columns and a target column. Optionally, this data can be split for segregating a validation or test dataset. \n",
    "You can use a registered MLTable in the workspace using the format '<mltable_name>:<version>' OR you can use a local file or folder as a MLTable. For e.g Input(mltable='my_mltable:1') OR Input(mltable=MLTable(local_path=\"./data\"))\n",
    "The parameter 'training_data' must always be provided.\n",
    "- `compute` - The compute on which the AutoML job will run. In this example we are using a compute called 'cpu-cluster' present in the workspace. You can replace it any other compute in the workspace. \n",
    "- `name` - The name of the Job/Run. This is an optional property. If not specified, a random name will be generated.\n",
    "- `experiment_name` - The name of the Experiment. An Experiment is like a folder with multiple runs in Azure ML Workspace that should be related to the same logical machine learning experiment.\n",
    "\n",
    "### set_limits() function parameters:\n",
    "This is an optional configuration method to configure limits parameters such as timeouts.     \n",
    "    \n",
    "- `timeout_minutes` - Maximum amount of time in minutes that the whole AutoML job can take before the job terminates. This timeout includes setup, featurization and training runs but does not include the ensembling and model explainability runs at the end of the process since those actions need to happen once all the trials (children jobs) are done. If not specified, the default job's total timeout is 6 days (8,640 minutes). To specify a timeout less than or equal to 1 hour (60 minutes), make sure your dataset's size is not greater than 10,000,000 (rows times column) or an error results.\n",
    "\n",
    "- `trial_timeout_minutes` - Maximum time in minutes that each trial (child job) can run for before it terminates. If not specified, a value of 1 month or 43200 minutes is used.\n",
    "    \n",
    "- `max_trials` - The maximum number of trials/runs each with a different combination of algorithm and hyperparameters to try during an AutoML job. If not specified, the default is 1000 trials. If using 'enable_early_termination' the number of trials used can be smaller.\n",
    "    \n",
    "- `max_concurrent_trials` - Represents the maximum number of trials (children jobs) that would be executed in parallel. It's a good practice to match this number with the number of nodes your cluster.\n",
    "    \n",
    "- `enable_early_termination` - Whether to enable early termination if the score is not improving in the short term. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# general job parameters\n",
    "max_trials = 5\n",
    "exp_name = \"dpv2-regression-experiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1634852262026
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "regression-configuration",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class RegressionJob: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    }
   ],
   "source": [
    "# Create the AutoML regression job with the related factory-function.\n",
    "\n",
    "regression_job = automl.regression(\n",
    "    compute=compute_name,\n",
    "    experiment_name=exp_name,\n",
    "    training_data=my_training_data_input,\n",
    "    target_column_name=\"ERP\",\n",
    "    primary_metric=\"R2Score\",\n",
    "    n_cross_validations=5,\n",
    "    enable_model_explainability=True,\n",
    "    tags={\"my_custom_tag\": \"My custom value\"},\n",
    ")\n",
    "\n",
    "# Limits are all optional\n",
    "regression_job.set_limits(\n",
    "    timeout_minutes=600,\n",
    "    trial_timeout_minutes=20,\n",
    "    max_trials=max_trials,\n",
    "    # max_concurrent_trials = 4,\n",
    "    # max_cores_per_trial: -1,\n",
    "    enable_early_termination=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure custom featurization\n",
    "# You can skip this cell to run AutoML using automatic featurization\n",
    "from azure.ai.ml.automl import ColumnTransformer\n",
    "\n",
    "transformer_params = {\n",
    "    \"imputer\": [\n",
    "        ColumnTransformer(fields=[\"CACH\"], parameters={\"strategy\": \"most_frequent\"}),\n",
    "        ColumnTransformer(fields=[\"PRP\"], parameters={\"strategy\": \"most_frequent\"}),\n",
    "    ],\n",
    "}\n",
    "regression_job.set_featurization(\n",
    "    mode=\"custom\",\n",
    "    transformer_params=transformer_params,\n",
    "    blocked_transformers=[\"LabelEncoder\"],\n",
    "    column_name_and_types={\"CHMIN\": \"Categorical\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Run the Command\n",
    "Using the `MLClient` created earlier, we will now run this Command in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1634852267930
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created job: RegressionJob({'log_verbosity': <LogVerbosity.INFO: 'Info'>, 'task_type': <TaskType.REGRESSION: 'Regression'>, 'environment_id': None, 'environment_variables': None, 'outputs': {}, 'type': 'automl', 'status': 'NotStarted', 'log_files': None, 'name': 'jolly_onion_tm659cl2bv', 'description': None, 'tags': {'my_custom_tag': 'My custom value'}, 'properties': {}, 'id': '/subscriptions/14585b9f-5c83-4a76-8055-42149123f99f/resourceGroups/azureml_rg/providers/Microsoft.MachineLearningServices/workspaces/azureml_dev_jamg/jobs/jolly_onion_tm659cl2bv', 'base_path': './', 'creation_context': <azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.SystemData object at 0x7feb303aa020>, 'serialize': <msrest.serialization.Serializer object at 0x7feb30dd8fa0>, 'inputs': {}, 'display_name': 'jolly_onion_tm659cl2bv', 'experiment_name': 'dpv2-regression-experiment', 'compute': 'cpu-cluster', 'services': {'Tracking': <azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.JobService object at 0x7feb303a9720>, 'Studio': <azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.JobService object at 0x7feb303aa380>}, 'resources': <azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.ResourceConfiguration object at 0x7feb303a9c00>, 'identity': None, 'data': <azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.TableVerticalDataSettings object at 0x7feb303a9db0>, 'featurization': <azure.ai.ml.entities._job.automl.tabular.featurization_settings.TabularFeaturizationSettings object at 0x7feb55ec4ca0>, 'limits': <azure.ai.ml.entities._job.automl.tabular.limit_settings.TabularLimitSettings object at 0x7feb30373370>, 'training': <azure.ai.ml.entities._job.automl.training_settings.RegressionTrainingSettings object at 0x7feb30dda9b0>, 'primary_metric': <RegressionPrimaryMetrics.R2_SCORE: 'R2Score'>})\n"
     ]
    }
   ],
   "source": [
    "# Submit the AutoML job\n",
    "returned_job = ml_client.jobs.create_or_update(\n",
    "    regression_job\n",
    ")  # submit the job to the backend\n",
    "\n",
    "print(f\"Created job: {returned_job}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait until the AutoML job is finished\n",
    "ml_client.jobs.stream(returned_job.name) waits until the specified job is finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: jolly_onion_tm659cl2bv\n",
      "Web View: https://ml.azure.com/runs/jolly_onion_tm659cl2bv?wsid=/subscriptions/14585b9f-5c83-4a76-8055-42149123f99f/resourcegroups/azureml_rg/workspaces/azureml_dev_jamg\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: jolly_onion_tm659cl2bv\n",
      "Web View: https://ml.azure.com/runs/jolly_onion_tm659cl2bv?wsid=/subscriptions/14585b9f-5c83-4a76-8055-42149123f99f/resourcegroups/azureml_rg/workspaces/azureml_dev_jamg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Wait for job to complete and stream updates\n",
    "ml_client.jobs.stream(returned_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://ml.azure.com/runs/jolly_onion_tm659cl2bv?wsid=/subscriptions/14585b9f-5c83-4a76-8055-42149123f99f/resourcegroups/azureml_rg/workspaces/azureml_dev_jamg&tid=16b3c013-d300-468d-ac64-7eda0820b6d3'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a URL for the status of the job\n",
    "returned_job.services[\"Studio\"].endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jolly_onion_tm659cl2bv\n"
     ]
    }
   ],
   "source": [
    "print(returned_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Retrieve the Best Trial (Best Model's trial/run)\n",
    "Use the MLFLowClient to access the results (such as Models, Artifacts, Metrics) of a previously completed AutoML Trial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize MLFlow Client\n",
    "The models and artifacts that are produced by AutoML can be accessed via the MLFlow interface. \n",
    "Initialize the MLFlow client here, and set the backend as Azure ML, via. the MLFlow Client.\n",
    "\n",
    "*IMPORTANT*, you need to have installed the latest MLFlow packages with:\n",
    "\n",
    "    pip install azureml-mlflow\n",
    "\n",
    "    pip install mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain the tracking URI for MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azureml://eastus.api.azureml.ms/mlflow/v1.0/subscriptions/14585b9f-5c83-4a76-8055-42149123f99f/resourceGroups/azureml_rg/providers/Microsoft.MachineLearningServices/workspaces/azureml_dev_jamg\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# Obtain the tracking URL from MLClient\n",
    "MLFLOW_TRACKING_URI = ml_client.workspaces.get(\n",
    "    name=ml_client.workspace_name\n",
    ").mlflow_tracking_uri\n",
    "\n",
    "print(MLFLOW_TRACKING_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current tracking uri: azureml://eastus.api.azureml.ms/mlflow/v1.0/subscriptions/14585b9f-5c83-4a76-8055-42149123f99f/resourceGroups/azureml_rg/providers/Microsoft.MachineLearningServices/workspaces/azureml_dev_jamg\n"
     ]
    }
   ],
   "source": [
    "# Set the MLFLOW TRACKING URI\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "\n",
    "print(\"\\nCurrent tracking uri: {}\".format(mlflow.get_tracking_uri()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking.client import MlflowClient\n",
    "\n",
    "# Initialize MLFlow client\n",
    "mlflow_client = MlflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the AutoML parent Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent Run: \n",
      "<Run: data=<RunData: metrics={'explained_variance': 0.8934360381091077,\n",
      " 'mean_absolute_error': 13.545632733661114,\n",
      " 'mean_absolute_percentage_error': 8.756661570034687,\n",
      " 'median_absolute_error': 2.46805419921875,\n",
      " 'normalized_mean_absolute_error': 0.011075742218856186,\n",
      " 'normalized_median_absolute_error': 0.0020180328693530257,\n",
      " 'normalized_root_mean_squared_error': 0.03397167276275391,\n",
      " 'normalized_root_mean_squared_log_error': 0.032270099470456816,\n",
      " 'r2_score': 0.8890376716187035,\n",
      " 'root_mean_squared_error': 41.54735578884804,\n",
      " 'root_mean_squared_log_error': 0.1403578669573778,\n",
      " 'spearman_correlation': 0.9825951205888839}, params={}, tags={'_azureml.ComputeTargetType': 'STANDARD_DS11_V2',\n",
      " 'automl_best_child_run_id': 'jolly_onion_tm659cl2bv_1',\n",
      " 'fit_time': '',\n",
      " 'iteration': '',\n",
      " 'mlflow.rootRunId': 'jolly_onion_tm659cl2bv',\n",
      " 'mlflow.runName': 'jolly_onion_tm659cl2bv',\n",
      " 'mlflow.user': 'Jose Medina Gomez',\n",
      " 'model_explain_best_run_child_id': 'jolly_onion_tm659cl2bv_1',\n",
      " 'model_explain_run': 'best_run',\n",
      " 'my_custom_tag': 'My custom value',\n",
      " 'pipeline_id_000': 'faf12f74cf9bbd358ca5525682c5030d36f7be7c;4bc4ec47eb8df2d5d68b361cd60120e65196f757;5cc37daec73ea64276ef956449645cdb519fdfc6;__AutoML_Ensemble__;__AutoML_Stack_Ensemble__',\n",
      " 'predicted_cost': '',\n",
      " 'run_algorithm': '',\n",
      " 'run_preprocessor': '',\n",
      " 'score': '',\n",
      " 'training_percent': ''}>, info=<RunInfo: artifact_uri='azureml://eastus.api.azureml.ms/mlflow/v2.0/subscriptions/14585b9f-5c83-4a76-8055-42149123f99f/resourceGroups/azureml_rg/providers/Microsoft.MachineLearningServices/workspaces/azureml_dev_jamg/experiments/62839098-29d1-492f-8724-1e810af4245a/runs/jolly_onion_tm659cl2bv/artifacts', end_time=1675911068196, experiment_id='62839098-29d1-492f-8724-1e810af4245a', lifecycle_stage='active', run_id='jolly_onion_tm659cl2bv', run_name='jolly_onion_tm659cl2bv', run_uuid='jolly_onion_tm659cl2bv', start_time=1675910383658, status='FINISHED', user_id='8321d1a2-2746-49fa-98eb-a2e6a7941583'>>\n"
     ]
    }
   ],
   "source": [
    "job_name = returned_job.name\n",
    "job_name = 'jolly_onion_tm659cl2bv'\n",
    "\n",
    "# Example if providing an specific Job name/ID\n",
    "# job_name = \"b4e95546-0aa1-448e-9ad6-002e3207b4fc\"\n",
    "\n",
    "# Get the parent run\n",
    "mlflow_parent_run = mlflow_client.get_run(job_name)\n",
    "\n",
    "print(\"Parent Run: \")\n",
    "print(mlflow_parent_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my_custom_tag': 'My custom value', 'model_explain_run': 'best_run', '_azureml.ComputeTargetType': 'STANDARD_DS11_V2', 'pipeline_id_000': 'faf12f74cf9bbd358ca5525682c5030d36f7be7c;4bc4ec47eb8df2d5d68b361cd60120e65196f757;5cc37daec73ea64276ef956449645cdb519fdfc6;__AutoML_Ensemble__;__AutoML_Stack_Ensemble__', 'score': '', 'predicted_cost': '', 'fit_time': '', 'training_percent': '', 'iteration': '', 'run_preprocessor': '', 'run_algorithm': '', 'automl_best_child_run_id': 'jolly_onion_tm659cl2bv_1', 'model_explain_best_run_child_id': 'jolly_onion_tm659cl2bv_1', 'mlflow.rootRunId': 'jolly_onion_tm659cl2bv', 'mlflow.runName': 'jolly_onion_tm659cl2bv', 'mlflow.user': 'Jose Medina Gomez'}\n"
     ]
    }
   ],
   "source": [
    "# Print parent run tags. 'automl_best_child_run_id' tag should be there.\n",
    "print(mlflow_parent_run.data.tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the AutoML best child run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found best child run id:  jolly_onion_tm659cl2bv_1\n",
      "Best child run: \n",
      "<Run: data=<RunData: metrics={'explained_variance': 0.8934360381091077,\n",
      " 'mean_absolute_error': 13.545632733661114,\n",
      " 'mean_absolute_percentage_error': 8.756661570034687,\n",
      " 'median_absolute_error': 2.46805419921875,\n",
      " 'normalized_mean_absolute_error': 0.011075742218856186,\n",
      " 'normalized_median_absolute_error': 0.0020180328693530257,\n",
      " 'normalized_root_mean_squared_error': 0.03397167276275391,\n",
      " 'normalized_root_mean_squared_log_error': 0.032270099470456816,\n",
      " 'r2_score': 0.8890376716187035,\n",
      " 'root_mean_squared_error': 41.54735578884804,\n",
      " 'root_mean_squared_log_error': 0.1403578669573778,\n",
      " 'spearman_correlation': 0.9825951205888839}, params={}, tags={'mlflow.parentRunId': 'jolly_onion_tm659cl2bv',\n",
      " 'mlflow.rootRunId': 'jolly_onion_tm659cl2bv',\n",
      " 'mlflow.runName': 'khaki_rocket_zpkbsfyb',\n",
      " 'mlflow.user': 'Jose Medina Gomez',\n",
      " 'model_explain_run_id': 'jolly_onion_tm659cl2bv_ModelExplain',\n",
      " 'model_explanation': 'True'}>, info=<RunInfo: artifact_uri='azureml://eastus.api.azureml.ms/mlflow/v2.0/subscriptions/14585b9f-5c83-4a76-8055-42149123f99f/resourceGroups/azureml_rg/providers/Microsoft.MachineLearningServices/workspaces/azureml_dev_jamg/experiments/62839098-29d1-492f-8724-1e810af4245a/runs/jolly_onion_tm659cl2bv_1/artifacts', end_time=1675910929147, experiment_id='62839098-29d1-492f-8724-1e810af4245a', lifecycle_stage='active', run_id='jolly_onion_tm659cl2bv_1', run_name='khaki_rocket_zpkbsfyb', run_uuid='jolly_onion_tm659cl2bv_1', start_time=1675910901052, status='FINISHED', user_id='8321d1a2-2746-49fa-98eb-a2e6a7941583'>>\n"
     ]
    }
   ],
   "source": [
    "# Get the best model's child run\n",
    "\n",
    "best_child_run_id = mlflow_parent_run.data.tags[\"automl_best_child_run_id\"]\n",
    "print(\"Found best child run id: \", best_child_run_id)\n",
    "\n",
    "best_run = mlflow_client.get_run(best_child_run_id)\n",
    "\n",
    "print(\"Best child run: \")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get best model run's metrics\n",
    "\n",
    "Access the results (such as Models, Artifacts, Metrics) of a previously completed AutoML Run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_absolute_percentage_error': 8.756661570034687,\n",
       " 'normalized_root_mean_squared_log_error': 0.032270099470456816,\n",
       " 'normalized_root_mean_squared_error': 0.03397167276275391,\n",
       " 'mean_absolute_error': 13.545632733661114,\n",
       " 'root_mean_squared_log_error': 0.1403578669573778,\n",
       " 'normalized_mean_absolute_error': 0.011075742218856186,\n",
       " 'explained_variance': 0.8934360381091077,\n",
       " 'r2_score': 0.8890376716187035,\n",
       " 'root_mean_squared_error': 41.54735578884804,\n",
       " 'spearman_correlation': 0.9825951205888839,\n",
       " 'normalized_median_absolute_error': 0.0020180328693530257,\n",
       " 'median_absolute_error': 2.46805419921875}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run.data.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the best model locally\n",
    "\n",
    "Access the results (such as Models, Artifacts, Metrics) of a previously completed AutoML Run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create local folder\n",
    "local_dir = \"./artifact_downloads\"\n",
    "if not os.path.exists(local_dir):\n",
    "    os.mkdir(local_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25018/2387391148.py:2: FutureWarning: ``mlflow.tracking.client.MlflowClient.download_artifacts`` is deprecated since 2.0. This method will be removed in a future release. Use ``mlflow.artifacts.download_artifacts`` instead.\n",
      "  local_path = mlflow_client.download_artifacts(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts downloaded in: /mnt/batch/tasks/shared/LS_root/mounts/clusters/jomedincpu/code/Users/jomedin/taxi-fare-predictions/notebooks/AutoML/artifact_downloads/outputs\n",
      "Artifacts: ['conda_env_v_1_0_0.yml', 'engineered_feature_names.json', 'env_dependencies.json', 'featurization_summary.json', 'generated_code', 'internal_cross_validated_models.pkl', 'mlflow-model', 'model.pkl', 'pipeline_graph.json', 'run_id.txt', 'scoring_file_pbi_v_1_0_0.py', 'scoring_file_v_1_0_0.py', 'scoring_file_v_2_0_0.py']\n"
     ]
    }
   ],
   "source": [
    "# Download run's artifacts/outputs\n",
    "local_path = mlflow_client.download_artifacts(\n",
    "    best_run.info.run_id, \"outputs\", local_dir\n",
    ")\n",
    "print(\"Artifacts downloaded in: {}\".format(local_path))\n",
    "print(\"Artifacts: {}\".format(os.listdir(local_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conda.yaml', 'MLmodel', 'model.pkl', 'python_env.yaml', 'requirements.txt']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the contents of the MLFlow model folder\n",
    "os.listdir(\"./artifact_downloads/outputs/mlflow-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Register Best Model and Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Create managed online endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    Model,\n",
    "    Environment,\n",
    "    CodeConfiguration,\n",
    "    ProbeSettings,\n",
    ")\n",
    "from azure.ai.ml.constants import ModelType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a unique endpoint name with current datetime to avoid conflicts\n",
    "import datetime\n",
    "\n",
    "online_endpoint_name = \"regression-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n",
    "\n",
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"this is a sample online endpoint for mlflow model\",\n",
    "    auth_mode=\"key\",\n",
    "    tags={\"foo\": \"bar\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ManagedOnlineEndpoint({'public_network_access': 'Enabled', 'provisioning_state': 'Succeeded', 'scoring_uri': 'https://regression-02090312349920.eastus.inference.ml.azure.com/score', 'swagger_uri': 'https://regression-02090312349920.eastus.inference.ml.azure.com/swagger.json', 'name': 'regression-02090312349920', 'description': 'this is a sample online endpoint for mlflow model', 'tags': {'foo': 'bar'}, 'properties': {'azureml.onlineendpointid': '/subscriptions/14585b9f-5c83-4a76-8055-42149123f99f/resourcegroups/azureml_rg/providers/microsoft.machinelearningservices/workspaces/azureml_dev_jamg/onlineendpoints/regression-02090312349920', 'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/14585b9f-5c83-4a76-8055-42149123f99f/providers/Microsoft.MachineLearningServices/locations/eastus/mfeOperationsStatus/oe:607be527-f51c-44df-a376-cb3876e5ac0b:f84d6e04-943c-495e-a01d-b179b71dd7fc?api-version=2022-02-01-preview'}, 'id': '/subscriptions/14585b9f-5c83-4a76-8055-42149123f99f/resourceGroups/azureml_rg/providers/Microsoft.MachineLearningServices/workspaces/azureml_dev_jamg/onlineEndpoints/regression-02090312349920', 'base_path': './', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f8da509efb0>, 'auth_mode': 'key', 'location': 'eastus', 'identity': <azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.ManagedServiceIdentity object at 0x7f8da509f460>, 'traffic': {}, 'mirror_traffic': {}, 'kind': 'Managed'})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.begin_create_or_update(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Register best model and deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"hardware-performance-model\"\n",
    "model = Model(\n",
    "    path=f\"azureml://jobs/{best_run.info.run_id}/outputs/artifacts/outputs/mlflow-model/\",\n",
    "    name=model_name,\n",
    "    description=\"my sample regression model\",\n",
    "    type=AssetTypes.MLFLOW_MODEL,\n",
    ")\n",
    "\n",
    "# for downloaded file\n",
    "# model = Model(path=\"artifact_downloads/outputs/model.pkl\", name=model_name)\n",
    "\n",
    "registered_model = ml_client.models.create_or_update(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/subscriptions/14585b9f-5c83-4a76-8055-42149123f99f/resourceGroups/azureml_rg/providers/Microsoft.MachineLearningServices/workspaces/azureml_dev_jamg/models/hardware-performance-model/versions/1'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "registered_model.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = ManagedOnlineDeployment(\n",
    "    name=\"hardware-performance-deploy\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=registered_model.id,\n",
    "    instance_type=\"Standard_DS3_V2\",\n",
    "    instance_count=1,\n",
    "    liveness_probe=ProbeSettings(\n",
    "        failure_threshold=30,\n",
    "        success_threshold=1,\n",
    "        timeout=2,\n",
    "        period=10,\n",
    "        initial_delay=2000,\n",
    "    ),\n",
    "    readiness_probe=ProbeSettings(\n",
    "        failure_threshold=10,\n",
    "        success_threshold=1,\n",
    "        timeout=10,\n",
    "        period=10,\n",
    "        initial_delay=2000,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint regression-02090312349920 exists\n",
      "Creating/updating online deployment hardware-performance-deploy "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done (40m 50s)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'result'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monline_deployments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin_create_or_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeployment\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'result'"
     ]
    }
   ],
   "source": [
    "ml_client.online_deployments.begin_create_or_update(deployment).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ManagedOnlineEndpoint({'public_network_access': 'Enabled', 'provisioning_state': 'Succeeded', 'scoring_uri': 'https://regression-02090312349920.eastus.inference.ml.azure.com/score', 'swagger_uri': 'https://regression-02090312349920.eastus.inference.ml.azure.com/swagger.json', 'name': 'regression-02090312349920', 'description': 'this is a sample online endpoint for mlflow model', 'tags': {'foo': 'bar'}, 'properties': {'azureml.onlineendpointid': '/subscriptions/14585b9f-5c83-4a76-8055-42149123f99f/resourcegroups/azureml_rg/providers/microsoft.machinelearningservices/workspaces/azureml_dev_jamg/onlineendpoints/regression-02090312349920', 'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/14585b9f-5c83-4a76-8055-42149123f99f/providers/Microsoft.MachineLearningServices/locations/eastus/mfeOperationsStatus/oe:607be527-f51c-44df-a376-cb3876e5ac0b:9703dcfc-2d85-4a6a-a3a9-680620282b39?api-version=2022-02-01-preview'}, 'id': '/subscriptions/14585b9f-5c83-4a76-8055-42149123f99f/resourceGroups/azureml_rg/providers/Microsoft.MachineLearningServices/workspaces/azureml_dev_jamg/onlineEndpoints/regression-02090312349920', 'base_path': './', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f8d63f070a0>, 'auth_mode': 'key', 'location': 'eastus', 'identity': <azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.ManagedServiceIdentity object at 0x7f8d63f06a40>, 'traffic': {'hardware-performance-deploy': 100}, 'mirror_traffic': {}, 'kind': 'Managed'})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hardware performance deployment to take 100% traffic\n",
    "endpoint.traffic = {\"hardware-performance-deploy\": 100}\n",
    "ml_client.begin_create_or_update(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorName</th>\n",
       "      <th>ModelName</th>\n",
       "      <th>MYCT</th>\n",
       "      <th>MMIN</th>\n",
       "      <th>MMAX</th>\n",
       "      <th>CACH</th>\n",
       "      <th>CHMIN</th>\n",
       "      <th>CHMAX</th>\n",
       "      <th>PRP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adviser</td>\n",
       "      <td>32/60</td>\n",
       "      <td>125</td>\n",
       "      <td>256</td>\n",
       "      <td>6000</td>\n",
       "      <td>256</td>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amdahl</td>\n",
       "      <td>470v/7</td>\n",
       "      <td>29</td>\n",
       "      <td>8000</td>\n",
       "      <td>32000</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amdahl</td>\n",
       "      <td>470v/7a</td>\n",
       "      <td>29</td>\n",
       "      <td>8000</td>\n",
       "      <td>32000</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amdahl</td>\n",
       "      <td>470v/7b</td>\n",
       "      <td>29</td>\n",
       "      <td>8000</td>\n",
       "      <td>32000</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amdahl</td>\n",
       "      <td>470v/7c</td>\n",
       "      <td>29</td>\n",
       "      <td>8000</td>\n",
       "      <td>16000</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  VendorName ModelName  MYCT  MMIN   MMAX  CACH  CHMIN  CHMAX  PRP\n",
       "0    adviser     32/60   125   256   6000   256     16    128  198\n",
       "1     amdahl    470v/7    29  8000  32000    32      8     32  269\n",
       "2     amdahl   470v/7a    29  8000  32000    32      8     32  220\n",
       "3     amdahl   470v/7b    29  8000  32000    32      8     32  172\n",
       "4     amdahl   470v/7c    29  8000  16000    32      8     16  132"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[198.99435424804688, 253.0391082763672, 252.9599151611328, 253.0175018310547, 131.99099731445312]'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the blue deployment with some sample data\n",
    "import pandas as pd\n",
    "\n",
    "test_data = pd.read_csv(\"./data/training-mltable-folder/training-machine-dataset.csv\").head(5)\n",
    "\n",
    "test_data = test_data.drop(\"ERP\", axis=1)\n",
    "\n",
    "test_data_json = test_data.to_json(orient=\"records\", indent=4)\n",
    "data = (\n",
    "    '{ \\\n",
    "          \"input_data\": {\"data\": '\n",
    "    + test_data_json\n",
    "    + \"}}\"\n",
    ")\n",
    "\n",
    "request_file_name = \"sample-request-hardware-performance.json\"\n",
    "\n",
    "with open(request_file_name, \"w\") as request_file:\n",
    "    request_file.write(data)\n",
    "\n",
    "# test the blue deployment with some sample data\n",
    "ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    deployment_name=\"hardware-performance-deploy\",\n",
    "    request_file=\"sample-request-hardware-performance.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{           \"input_data\": {\"data\": [\n",
      "    {\n",
      "        \"VendorName\":\"adviser\",\n",
      "        \"ModelName\":\"32\\/60\",\n",
      "        \"MYCT\":125,\n",
      "        \"MMIN\":256,\n",
      "        \"MMAX\":6000,\n",
      "        \"CACH\":256,\n",
      "        \"CHMIN\":16,\n",
      "        \"CHMAX\":128,\n",
      "        \"PRP\":198\n",
      "    },\n",
      "    {\n",
      "        \"VendorName\":\"amdahl\",\n",
      "        \"ModelName\":\"470v\\/7\",\n",
      "        \"MYCT\":29,\n",
      "        \"MMIN\":8000,\n",
      "        \"MMAX\":32000,\n",
      "        \"CACH\":32,\n",
      "        \"CHMIN\":8,\n",
      "        \"CHMAX\":32,\n",
      "        \"PRP\":269\n",
      "    },\n",
      "    {\n",
      "        \"VendorName\":\"amdahl\",\n",
      "        \"ModelName\":\"470v\\/7a\",\n",
      "        \"MYCT\":29,\n",
      "        \"MMIN\":8000,\n",
      "        \"MMAX\":32000,\n",
      "        \"CACH\":32,\n",
      "        \"CHMIN\":8,\n",
      "        \"CHMAX\":32,\n",
      "        \"PRP\":220\n",
      "    },\n",
      "    {\n",
      "        \"VendorName\":\"amdahl\",\n",
      "        \"ModelName\":\"470v\\/7b\",\n",
      "        \"MYCT\":29,\n",
      "        \"MMIN\":8000,\n",
      "        \"MMAX\":32000,\n",
      "        \"CACH\":32,\n",
      "        \"CHMIN\":8,\n",
      "        \"CHMAX\":32,\n",
      "        \"PRP\":172\n",
      "    },\n",
      "    {\n",
      "        \"VendorName\":\"amdahl\",\n",
      "        \"ModelName\":\"470v\\/7c\",\n",
      "        \"MYCT\":29,\n",
      "        \"MMIN\":8000,\n",
      "        \"MMAX\":16000,\n",
      "        \"CACH\":32,\n",
      "        \"CHMIN\":8,\n",
      "        \"CHMAX\":16,\n",
      "        \"PRP\":132\n",
      "    }\n",
      "]}}\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait and delete endpoint\n",
    "import time\n",
    "\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get endpoint details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hardware-performance-deploy': 100}\n",
      "https://regression-02090312349920.eastus.inference.ml.azure.com/score\n"
     ]
    }
   ],
   "source": [
    "# Get the details for online endpoint\n",
    "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
    "\n",
    "# existing traffic details\n",
    "print(endpoint.traffic)\n",
    "\n",
    "# Get the scoring URI\n",
    "print(endpoint.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the deployment and endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_endpoints.begin_delete(name=online_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Step: Load the best model and try predictions\n",
    "\n",
    "Loading the models locally assume that you are running the notebook in an environment compatible with the model. The list of dependencies that is expected by the model is specified in the MLFlow model produced by AutoML (in the 'conda.yaml' file within the mlflow-model folder).\n",
    "\n",
    "Since the AutoML model was trained remotelly in a different environment with different dependencies to your current local conda environment where you are running this notebook, if you want to load the model you have several options:\n",
    "\n",
    "1. A recommended way to locally load the model in memory and try predictions is to create a new/clean conda environment with the dependencies specified in the conda.yml file within the MLFlow model's folder, then use MLFlow to load the model and call .predict() as explained in the notebook **mlflow-model-local-inference-test.ipynb** in this same folder.\n",
    "\n",
    "2. You can install all the packages/dependencies specified in conda.yml into your current conda environment you used for using Azure ML SDK and AutoML. MLflow SDK also have a method to install the dependencies in the current environment. However, this option could have risks of package version conflicts depending on what's installed in your current environment.\n",
    "\n",
    "3. You can also use: mlflow models serve -m 'xxxxxxx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "You can see further examples of other AutoML tasks such as Image-Classification, Image-Object-Detection, NLP-Text-Classification, Time-Series-Forcasting, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "Python 3.10 - SDK V2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
